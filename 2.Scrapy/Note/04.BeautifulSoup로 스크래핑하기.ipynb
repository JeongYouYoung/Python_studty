{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BeautifulSoup\n",
    "- 간단하게 HTML과 XML에서 정보를 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BeautifulSoup 기본 사용법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "<head>\n",
      "</head>\n",
      "<body>\n",
      "<h1>스크레이핑이란?</h1>\n",
      "<p>웹 페이지를 분석하는 것</p>\n",
      "<p>원하는 부분을 추출하는 것</p>\n",
      "</body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "# Library\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "# Site Address\n",
    "url = \"https://zeushahn.github.io/Test/python/bs_exam01.html\"\n",
    "\n",
    "# urlOpen\n",
    "res = req.urlopen(url)\n",
    "\n",
    "# BeautifulSoup(한테 일시켜) 으로 분석하기\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1= 스크레이핑이란?\n",
      "p1= 웹 페이지를 분석하는 것\n",
      "p2= 원하는 부분을 추출하는 것\n"
     ]
    }
   ],
   "source": [
    "# 원하는 부분 추출하기\n",
    "h1 = soup.html.body.h1 # html밑에 ,body 밑에 , h1부분!\n",
    "print(\"h1=\", h1.string) #.string -> 태그없애기 문자 구성에따라 달라져서 string 가 안되면 .text로 써봐\n",
    "\n",
    "p1 = soup.html.body.p\n",
    "print(\"p1=\", p1.string) \n",
    "\n",
    "p2 = p1.next_sibling.next_sibling # 같은 레벨의 동등한 위치에 있는 값을 가져옴 next_sibling를 두번 쓰는건 \\n(줄바꿈)떄문에 두번쓰는거\n",
    "print(\"p2=\", p2.string) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# id로 요소 찾는 방법\n",
    "\n",
    ">id가 있으면 id값 부터 찾는게 좋음!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "<head>\n",
      "</head>\n",
      "<body>\n",
      "<h1 id=\"title\">스크레이핑이란?</h1>\n",
      "<p id=\"body\">웹 페이지를 분석하는 것</p>\n",
      "<p>원하는 부분을 추출하는 것</p>\n",
      "</body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "# Library\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "# Site Address\n",
    "url = \"https://zeushahn.github.io/Test/python/bs_exam02.html\"\n",
    "\n",
    "# urlOpen\n",
    "res = req.urlopen(url)\n",
    "\n",
    "# BeautifulSoup(한테 일시켜) 으로 분석하기\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id 값이 title : 스크레이핑이란?\n",
      "h1 : 스크레이핑이란?\n",
      "id 값이 body : 웹 페이지를 분석하는 것\n"
     ]
    }
   ],
   "source": [
    "title = soup.find(id=\"title\")\n",
    "print(\"id 값이 title :\",title.string)\n",
    "\n",
    "title = soup.find('h1')\n",
    "print(\"h1 :\",title.string)\n",
    "\n",
    "body = soup.find(id=\"body\")\n",
    "print(\"id 값이 body :\",body.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 여러개의 요소 추출하기\n",
    "\n",
    "> 하이퍼링크 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "<head>\n",
      "</head>\n",
      "<body>\n",
      "<ul>\n",
      "<li><a href=\"http://www.naver.com\">naver</a></li>\n",
      "<li><a href=\"http://www.daum.net\">daum</a></li>\n",
      "</ul>\n",
      "</body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "# Library\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "# Site Address\n",
    "url = \"https://zeushahn.github.io/Test/python/bs_exam03.html\"\n",
    "\n",
    "# urlOpen\n",
    "res = req.urlopen(url)\n",
    "\n",
    "# BeautifulSoup(한테 일시켜) 으로 분석하기\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naver > http://www.naver.com\n",
      "daum > http://www.daum.net\n"
     ]
    }
   ],
   "source": [
    "links = soup.find_all(\"a\")# a태그 안에 있는걸 다 가지고 올꺼야~!\n",
    "# print(links) # List로 들고오네! for문 쓰면 되겠네!\n",
    "\n",
    "for a in links:\n",
    "    # print(a)\n",
    "    text = a.string\n",
    "    href = a.attrs['href']\n",
    "    print(text, \">\", href)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기상청 RSS에서 필요한 정보만 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "# Site Address\n",
    "url = \"http://www.kma.go.kr/weather/forecast/mid-term-rss3.jsp?stnId=109\"\n",
    "\n",
    "# urlOpen\n",
    "res = req.urlopen(url)\n",
    "\n",
    "# BeautifulSoup(한테 일시켜) 으로 분석하기\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "#print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (강수) 30일(목)~7월 1일(금)은 흐리고 비가 내리겠습니다. \n",
      " (기온) 이번 예보기간 아침 기온은 21~23도로 어제(26일, 아침최저기온 24~25도)보다 낮겠고, 낮 기온은 26~31도로 어제(낮최고기온 27~31도)와 비슷하겠습니다.\n",
      " (해상) 서해중부해상의 물결은 30일(목) 오전까지 1.0~2.5m로 일겠고, 그 밖의 날은 1.0~2.0m로 일겠습니다.\n",
      " (주말전망) 7월 2일(토)은 대체로 흐리겠고, 7월 3일(일)은 구름많겠습니다. 아침 기온은 21~23도, 낮 기온은 27~31도가 되겠습니다. \n",
      "\n",
      "* 7월 1일(금)에는 정체전선의 위치에 따라 강수 구역이 변동될 수 있으며 정체전선의 영향권에서 벗어난 지역에서도 대기 불안정으로 소나기가 내리는 곳이 있겠고,\n",
      "  7월 2일(토) 이후에는 강수 예보가 없으나 주변 기압계의 세기와 흐름에 따라 강수 예보가 변경될 가능성이 있겠으니, 앞으로 발표되는 예보와 기상정보를 참고하기 바랍니다.\n"
     ]
    }
   ],
   "source": [
    "# title과 wf만 가져오겠다\n",
    "# link = soup.find(\"title\").split('<title>')\n",
    "link1 = soup.find(\"wf\").string.replace(\"○\",\"\").replace(\"<br />\",\"\\n\")\n",
    "print(link1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSS 선택자 사용하기\n",
    "- BeautifulSoup는 자바스크립트 라이브러리인 JQuery처럼 CSS선택자를 지정해서 요소 추출 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "<head>\n",
      "</head>\n",
      "<body>\n",
      "<div id=\"meigen\">\n",
      "<h1>위키북스 도서</h1>\n",
      "<ul class=\"items\">\n",
      "<li>유니티 게임 이펙트 입문</li>\n",
      "<li>스위프트로 시작하는 아이폰 앱 개발 교과서</li>\n",
      "<li>모던 웹사이트 디자인의 정석</li>\n",
      "</ul>\n",
      "</div>\n",
      "</body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "# Library\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "# Site Address\n",
    "url = \"https://zeushahn.github.io/Test/python/bs_exam04.html\"\n",
    "\n",
    "# urlOpen\n",
    "res = req.urlopen(url)\n",
    "\n",
    "# BeautifulSoup(한테 일시켜) 으로 분석하기\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "print(soup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>위키북스 도서</h1>\n"
     ]
    }
   ],
   "source": [
    "# 필요한 부분을 CSS 쿼리로 추출하기 (# : id | . : class | > : 자식 | 빈칸 : 후손)\n",
    "# 소스를 보면 body입장에서 div는 내 자식이고 나머지는 다 후손임\n",
    "# 맨뒤에부터 적는게 제일 편해\n",
    "\n",
    "h1 = soup.select_one(\"#meigen > h1\") # div 아이디는 meigen이고 div의 자식은 h1 이다.\n",
    "print(h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<li>유니티 게임 이펙트 입문</li>, <li>스위프트로 시작하는 아이폰 앱 개발 교과서</li>, <li>모던 웹사이트 디자인의 정석</li>]\n"
     ]
    }
   ],
   "source": [
    "li_list = soup.select(\"div#meigen > ul.items > li\") # ul이 여러개인데 각자 class가 있어\n",
    "print(li_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "유니티 게임 이펙트 입문\n",
      "스위프트로 시작하는 아이폰 앱 개발 교과서\n",
      "모던 웹사이트 디자인의 정석\n"
     ]
    }
   ],
   "source": [
    "for li in li_list:\n",
    "    print(li.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 네이버 금융에서 환율 정보 추출하기\n",
    "- https://finance.naver.com/marketindex/  \n",
    "- :에서 도구더보기 -> 개발자도구 -> elemnt -> 마우스로 가져올 정보 클릭하고 -> 우클릭 -> copy -> copy selecter\n",
    "### 미국 환율 가져오기\n",
    "- #exchangeList > li.on > a.head.usd > div > span.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "# Site Address\n",
    "url = \"https://finance.naver.com/marketindex/\"\n",
    "\n",
    "# urlOpen\n",
    "res = req.urlopen(url)\n",
    "\n",
    "# BeautifulSoup(한테 일시켜) 으로 분석하기\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "# print(soup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,288.00\n"
     ]
    }
   ],
   "source": [
    "price = soup.select_one(\"#exchangeList > li.on > a.head.usd > div > span.value\").string # ul이 여러개인데 각자 class가 있어\n",
    "print(price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise \n",
    "# 미국, 일본, 유럽엽합, 중국의 환율 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Library\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "# Site Address\n",
    "url = \"https://finance.naver.com/marketindex/\"\n",
    "\n",
    "# urlOpen\n",
    "res = req.urlopen(url)\n",
    "\n",
    "# BeautifulSoup(한테 일시켜) 으로 분석하기\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "#print(soup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,288.00\n",
      "957.30\n",
      "1,360.39\n",
      "192.67\n",
      "135.2000\n",
      "1.0560\n",
      "1.2271\n",
      "103.9500\n",
      "107.62\n",
      "2131.77\n",
      "1827.0\n",
      "75858.24\n"
     ]
    }
   ],
   "source": [
    "# 풀이\n",
    "# 불러와서 공통점을 찾자\n",
    "##exchangeList > li.on > a.head.usd > div > span.value\n",
    "##exchangeList > li.on > a.head.jpy > div > span.value\n",
    "##exchangeList > li.on > a.head.eur > div > span.value\n",
    "#1번 span.value -> div ->\n",
    "price3 = soup.select(\"span.value\")\n",
    "\n",
    "for price in price3:\n",
    "    print(price.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<span class=\"value\">1,288.00</span>, <span class=\"value\">957.30</span>, <span class=\"value\">1,360.39</span>, <span class=\"value\">192.67</span>]\n"
     ]
    }
   ],
   "source": [
    "# 필요한 부분을 CSS 쿼리로 추출하기 (# : id // . : class // > : 자식 //  빈칸 : 후손)\n",
    "price = soup.select(\"#exchangeList span.value\")\n",
    "print(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1,288.00', '957.30', '1,360.39', '192.67']\n"
     ]
    }
   ],
   "source": [
    "# 가져온 값 리스트에 저장하기\n",
    "price_list = []\n",
    "for span in price:\n",
    "    price_list.append(span.text)\n",
    "print(price_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "미국 :   1,288.00\n",
      "일본 :     957.30\n",
      "유럽연합 :   1,360.39\n",
      "중국 :     192.67\n"
     ]
    }
   ],
   "source": [
    "list_world = [\"미국\",\"일본\",\"유럽연합\",\"중국\"]\n",
    "\n",
    "for i in range(len(price_list)):\n",
    "    print(\"%s : %10s\"%(list_world[i],price_list[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<h3 class=\"h_lst\"><span class=\"blind\">미국 USD</span></h3>,\n",
       " <h3 class=\"h_lst\"><span class=\"blind\">일본 JPY(100엔)</span></h3>,\n",
       " <h3 class=\"h_lst\"><span class=\"blind\">유럽연합 EUR</span></h3>,\n",
       " <h3 class=\"h_lst\"><span class=\"blind\">중국 CNY</span></h3>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 풀이2) 나라이름 뽑아서 리스트에 넣기/ 환율값 리스트에 저장하기\n",
    "price4 = soup.select(\"#exchangeList > li > a.head > h3\")\n",
    "price4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['미국 USD', '일본 JPY(100엔)', '유럽연합 EUR', '중국 CNY']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_world2 = []\n",
    "for h3 in price4:\n",
    "    list_world2.append(h3.text)\n",
    "list_world2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "미국 USD :   1,288.00\n",
      "일본 JPY(100엔) :     957.30\n",
      "유럽연합 EUR :   1,360.39\n",
      "중국 CNY :     192.67\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(price_list)):\n",
    "    print(\"%s : %10s\"%(list_world2[i],price_list[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<span class=\"value\">1,288.00</span>, <span class=\"value\">957.30</span>, <span class=\"value\">1,360.39</span>, <span class=\"value\">192.67</span>]\n",
      "[<span class=\"blind\">미국 USD</span>, <span class=\"blind\">일본 JPY(100엔)</span>, <span class=\"blind\">유럽연합 EUR</span>, <span class=\"blind\">중국 CNY</span>]\n",
      "미국 \t:   1,288.00\n",
      "일본 \t:     957.30\n",
      "유럽연합 \t:   1,360.39\n",
      "중국 \t:     192.67\n"
     ]
    }
   ],
   "source": [
    "price_list = soup.select(\"ul#exchangeList  span.value\")\n",
    "print(price_list)\n",
    "nation_list = soup.select(\"ul#exchangeList h3.h_lst  span.blind\")\n",
    "print(nation_list)\n",
    "for i in range(len(price_list)):\n",
    "  print(\"%s \\t:\"%(nation_list[i].string.split()[0]), price_list[i].string.rjust(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 제목 글자 가져오기!\n",
    " - https://movie.daum.net/boxoffice/yearly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Library\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "# Site Address\n",
    "url = \"https://movie.daum.net/ranking/boxoffice/yearly\"\n",
    "\n",
    "# urlOpen\n",
    "res = req.urlopen(url)\n",
    "\n",
    "# BeautifulSoup(한테 일시켜) 으로 분석하기\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "#print(soup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1 : 스파이더맨: 노 웨이 홈\n",
      "  2 : 모가디슈\n",
      "  3 : 이터널스\n",
      "  4 : 블랙 위도우\n",
      "  5 : 분노의 질주: 더 얼티메이트\n",
      "  6 : 싱크홀\n",
      "  7 : 극장판 귀멸의 칼날: 무한열차편\n",
      "  8 : 베놈 2: 렛 데어 비 카니지\n",
      "  9 : 소울\n",
      " 10 : 크루엘라\n",
      " 11 : 샹치와 텐 링즈의 전설\n",
      " 12 : 인질\n",
      " 13 : 듄\n",
      " 14 : 보이스\n",
      " 15 : 007 노 타임 투 다이\n",
      " 16 : 미나리\n",
      " 17 : 발신제한\n",
      " 18 : 보스 베이비 2\n",
      " 19 : 콰이어트 플레이스 2\n",
      " 20 : 랑종\n",
      " 21 : 유체이탈자\n",
      " 22 : 컨저링3: 악마가 시켰다\n",
      " 23 : 기적\n",
      " 24 : 고질라 VS. 콩\n",
      " 25 : 킹스맨: 퍼스트 에이전트\n",
      " 26 : 엔칸토: 마법의 세계\n",
      " 27 : 연애 빠진 로맨스\n",
      " 28 : 장르만 로맨스\n",
      " 29 : 미션 파서블\n",
      " 30 : 더 수어사이드 스쿼드\n",
      " 31 : 비와 당신의 이야기\n",
      " 32 : 서복\n",
      " 33 : 킬러의 보디가드 2\n",
      " 34 : 루카\n",
      " 35 : 자산어보\n",
      " 36 : 내일의 기억\n",
      " 37 : 라야와 마지막 드래곤\n",
      " 38 : 프리 가이 \n",
      " 39 : 더 스파이\n",
      " 40 : 강릉\n",
      " 41 : 정글 크루즈\n",
      " 42 : 명탐정 코난: 비색의 탄환\n",
      " 43 : 캐시트럭\n",
      " 44 : 크루즈 패밀리: 뉴 에이지\n",
      " 45 : 이스케이프 룸 2: 노 웨이 아웃\n",
      " 46 : 극장판 포켓몬스터: 정글의 아이, 코코\n",
      " 47 : 극장판 짱구는 못말려: 격돌! 낙서왕국과 얼추 네 명의 용사들\n",
      " 48 : 매트릭스: 리저렉션\n",
      " 49 : 방법: 재차의\n",
      " 50 : 새해전야\n"
     ]
    }
   ],
   "source": [
    "#mainContent > div > div.box_boxoffice > ol > li:nth-child(1) > div > div.thumb_cont > strong > a\n",
    "#mainContent > div > div.box_boxoffice > ol > li:nth-child(1) > div > div.thumb_cont > strong > a\n",
    "\n",
    "title = soup.select(\"strong > a\")\n",
    "#print(title)\n",
    "num = 0\n",
    "for a in title:\n",
    "    list = a.string\n",
    "    num += 1 \n",
    "    print(\"%3d : %s\"%(num,list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이미지 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mainContent > div > div.box_boxoffice > ol > li:nth-child(1) > div > div.thumb_item > div.poster_movie > img\n",
    "#mainContent > div > div.box_boxoffice > ol > li:nth-child(2) > div > div.thumb_item > div.poster_movie > img\n",
    "title = soup.select(\"div.poster_movie > img\")\n",
    "#print(title)\n",
    "\n",
    "\n",
    "for src in title:\n",
    "    text = src.attrs['alt']\n",
    "    href = src.attrs['src']\n",
    "   \n",
    "    # print(text)\n",
    "    # print(href)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#라이브러리 불러오기\n",
    "import urllib.request as request\n",
    "\n",
    "#다운로드 파일이름 결정\n",
    "for i in range(len(title)):\n",
    "    url = title[i].attrs['src']\n",
    "    saveName = f\"../Data/{title[i].attrs['alt']}.jpeg\"\n",
    "    request.urlretrieve(url, saveName)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 다음 IT News 많이 본 뉴스 제목 크롤링\n",
    "\n",
    "- https://media.daum.net/digital/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Library\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "# Site Address\n",
    "url = \"https://media.daum.net/digital/\"\n",
    "\n",
    "# urlOpen\n",
    "res = req.urlopen(url)\n",
    "\n",
    "# BeautifulSoup(한테 일시켜) 으로 분석하기\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "#print(soup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[IT 뉴스 제목 크롤링]\n",
      " 1 :  \"아이폰14 프로 AOD, 새 잠금화면 위젯 지원\"\n",
      " 2 :  6월에 출시한 디아블로 이모탈-우마무스메-미르M, 3개 작품의 인기 비결은?\n",
      " 3 :  화웨이, '100W 터보' 고속 충전 기술 탑재 폰 곧 공개\n",
      " 4 :  中 손목용 웨어러블 시장서 애플만 '나홀로' 성장\n",
      " 5 :  비트코인 팔아치우는 채굴자들..\"바닥 가까워졌다는 신호?\"\n",
      " 6 :  [단독]'각 세종'까지 찾았다..네이버 이해진의 남다른 '자국 데이터' 집념\n",
      " 7 :  [생활속 과학이야기]  달에 대한 상상과 우주탐사\n",
      " 8 :  \"죽음 두려워요\"..감정 AI 논쟁에 숨어든 빅테크의 속임수\n",
      " 9 :  카겜즈, 오딘-우마 '원투펀치' 완성..남궁훈도 인정한 조계현 대표의 '안목'\n",
      "10 :  폭락장에 상장 나선 쏘카..창업주 이재웅 \"최소 1년간 주식 안판다\"\n"
     ]
    }
   ],
   "source": [
    "#body > div.container-doc.cont-category > main > section > div.main-sub > div.box_g.box_news_major > ul > li:nth-child(1) > strong\n",
    "#body > div.container-doc.cont-category > main > section > div.main-sub > div.box_g.box_news_major > ul > li:nth-child(2) > strong > a\n",
    "title = soup.select(\"li > strong > a\")\n",
    "# print(title)\n",
    "\n",
    "# 뉴스제목 리스트에 넣기\n",
    "it_title = []\n",
    "num = 0\n",
    "for a in title:\n",
    "    it_title.append(a.text)\n",
    "\n",
    "print(\"[IT 뉴스 제목 크롤링]\")\n",
    "for i in range(len(it_title)):\n",
    "    num += 1\n",
    "    print(\"%2d : \"%num,it_title[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
